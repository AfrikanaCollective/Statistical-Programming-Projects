{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import string\n",
    "import tweepy\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import preprocessor as pre\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pandas import Panel\n",
    "tqdm.pandas()\n",
    "pd.set_option('max_colwidth',500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "ROOT_DATA = 'D:/Statistical Programming Projects/Social Network and Sentiment Analysis/data/'\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9,6),'lines.linewidth': 5, 'lines.markersize': 10})\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\",{\"font.family\": [\"Corbel\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Twitter Dev credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT_DATA+'twitter.txt', 'r') as textFile:\n",
    "    for line in textFile:\n",
    "        exec(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True,timeout=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set date parameters and file save location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare file paths as follows for three files\n",
    "uhc_tweets = ROOT_DATA+\"uhc_data.csv\"\n",
    "\n",
    "#set two date variables for date range\n",
    "start_date = datetime.datetime(2020, 4, 18)\n",
    "end_date = datetime.datetime(2020, 4, 29)\n",
    "\n",
    "COLS = ['id', #Unique id of the tweet\n",
    "        'tweeter_handle', #Twerp\n",
    "        'init_reach', #Followers\n",
    "        'timestamp', # Tweet timestamp\n",
    "        'orig_tweet', #Actual text\n",
    "        'likes', #Favourates of the tweet\n",
    "        'retweets', # Retweet count\n",
    "        'hashtags', #Tagging used\n",
    "        'mentions', #Profiles mentioned\n",
    "        'location', # Twerp's location\n",
    "        'retweeted', #Tweet that's being retweeted\n",
    "        'reply' #Tweet Twerp is replying to\n",
    "       ]\n",
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to scrap and tabulate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets(keyword, file):\n",
    "    \n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "    \n",
    "    if os.path.exists(file):\n",
    "        health_tweets = pd.read_csv(file, header=0)\n",
    "    else:\n",
    "        health_tweets = pd.DataFrame(columns=COLS)\n",
    "        health_tweets.to_csv(file, columns=COLS, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    for page in tweepy.Cursor(api.search\n",
    "                            ,q=keyword\n",
    "                            ,tweet_mode='extended'\n",
    "                            ,include_rts=True\n",
    "                            ,count=1000000\n",
    "                            ,lang = \"en\"\n",
    "                            ,since=start_date\n",
    "                            ,until=end_date).pages(100000000000000):        \n",
    "        \n",
    "        \n",
    "        for status in page:           \n",
    "                        \n",
    "            status = status._json            \n",
    "            retweeted_tweet = None\n",
    "            media = False\n",
    "            tweet_reply = None\n",
    "            aberrant = False\n",
    "\n",
    "            if(status['full_text'].startswith('RT @') or ('quoted_status' in status.keys())):\n",
    "                \n",
    "                #If the tweet is a reply\n",
    "                if 'in_reply_to_status_id_str' in status.keys():\n",
    "                    tweet_reply = status['in_reply_to_status_id_str']\n",
    "                \n",
    "                #Add/update retweeted tweet\n",
    "                if 'quoted_status' in status.keys():\n",
    "                    embedded_key = 'quoted_status'\n",
    "                    retweeted_tweet = status[embedded_key]['id_str']\n",
    "                elif status['full_text'].startswith('RT @'):\n",
    "                    embedded_key = 'retweeted_status'\n",
    "                    try:\n",
    "                        retweeted_tweet = status[embedded_key]['id_str']\n",
    "                    except KeyError:\n",
    "                        media=True\n",
    "                        try:\n",
    "                            retweeted_tweet = [details['source_status_id_str'] for details in \\\n",
    "                                        status['entities']['media'] if 'source_status_id' in details.keys()][0]\n",
    "                        except KeyError:\n",
    "                            aberrant = True\n",
    "                        except IndexError:\n",
    "                            pp.pprint(status)\n",
    "                \n",
    "                #      \n",
    "                #Save the retweet/quoted tweet\n",
    "                #\n",
    "                if not media: #Just a text retweet, can use embedded key\n",
    "                    retweeted_likes = status[embedded_key]['favorite_count']\n",
    "                    retweeted_shares = status[embedded_key]['retweet_count']               \n",
    "\n",
    "                    #Hashtags\n",
    "                    hashtags = \", \".join([hashtag_item['text']\\\n",
    "                                for hashtag_item in status[embedded_key]['entities']['hashtags']])\n",
    "\n",
    "                    #Mentions\n",
    "                    mentions = \", \".join([mention['screen_name']\\\n",
    "                                for mention in status[embedded_key]['entities']['user_mentions']])                \n",
    "                \n",
    "                    this_tweet={\n",
    "                        'id':retweeted_tweet\n",
    "                        ,'tweeter_handle':status[embedded_key]['user']['screen_name']\n",
    "                        ,'init_reach':status[embedded_key]['user']['followers_count']\n",
    "                        ,'timestamp':status[embedded_key]['created_at']\n",
    "                        ,'orig_tweet':status[embedded_key]['full_text']                               \n",
    "                        ,'likes':retweeted_likes\n",
    "                        ,'retweets':retweeted_shares \n",
    "                        ,'hashtags':hashtags\n",
    "                        ,'mentions':mentions\n",
    "                        ,'location':status[embedded_key]['user']['location']\n",
    "                        ,'retweeted':None\n",
    "                        ,'reply':None\n",
    "                    }\n",
    "\n",
    "                    with open(file=file,mode=\"a\",encoding='utf-8') as csvfile:\n",
    "                        writer = csv.DictWriter(csvfile,fieldnames=COLS,lineterminator = '\\n')\n",
    "                        writer.writerow(this_tweet)\n",
    "                        \n",
    "                #\n",
    "                #Save the quoting tweet/retweet action        \n",
    "                #\n",
    "                #Hashtags\n",
    "                hashtags = \", \".join([hashtag_item['text'] \\\n",
    "                                      for hashtag_item in status['entities']['hashtags']])\n",
    "                #Mentions\n",
    "                mentions = \", \".join([mention['screen_name'] \\\n",
    "                                      for mention in status['entities']['user_mentions']])\n",
    "                this_tweet={\n",
    "                    'id':status['id_str']\n",
    "                    ,'tweeter_handle':status['user']['screen_name']\n",
    "                    ,'init_reach':status['user']['followers_count']\n",
    "                    ,'timestamp':status['created_at']\n",
    "                    ,'orig_tweet': '' if status['full_text'].startswith('RT @') and not aberrant else status['full_text']                               \n",
    "                    ,'likes':0 if status['full_text'].startswith('RT @') else status['favorite_count']\n",
    "                    ,'retweets':0 if status['full_text'].startswith('RT @') else status['retweet_count'] \n",
    "                    ,'hashtags':'' if status['full_text'].startswith('RT @') else hashtags\n",
    "                    ,'mentions':'' if status['full_text'].startswith('RT @') and not aberrant else mentions\n",
    "                    ,'location':status['user']['location']\n",
    "                    ,'retweeted':retweeted_tweet\n",
    "                    ,'reply': tweet_reply \n",
    "                }\n",
    "\n",
    "                with open(file=file,mode=\"a\",encoding='utf-8') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile,fieldnames=COLS,lineterminator = '\\n')\n",
    "                    writer.writerow(this_tweet)\n",
    "                \n",
    "                \n",
    "            else: #Original tweet\n",
    "                #Hashtags\n",
    "                hashtags = \", \".join([hashtag_item['text'] \\\n",
    "                                      for hashtag_item in status['entities']['hashtags']])\n",
    "                #Mentions\n",
    "                mentions = \", \".join([mention['screen_name'] \\\n",
    "                                      for mention in status['entities']['user_mentions']])                \n",
    "                this_tweet={\n",
    "                        'id':status['id_str']\n",
    "                        ,'tweeter_handle':status['user']['screen_name']\n",
    "                        ,'init_reach':status['user']['followers_count']\n",
    "                        ,'timestamp':status['created_at']\n",
    "                        ,'orig_tweet':status['full_text']                               \n",
    "                        ,'likes':status['favorite_count']\n",
    "                        ,'retweets':status['retweet_count'] \n",
    "                        ,'hashtags':hashtags\n",
    "                        ,'mentions':mentions\n",
    "                        ,'location':status['user']['location']\n",
    "                        ,'retweeted':None\n",
    "                        ,'reply':None\n",
    "                }\n",
    "                \n",
    "                with open(file=file,mode=\"a\",encoding='utf-8') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile,fieldnames=COLS,lineterminator = '\\n')\n",
    "                    writer.writerow(this_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare keywords as a query for three categories\n",
    "uhc_keywords = '(covid OR covid19 OR coronavirus OR cov19 OR cov-19)'\n",
    "try:\n",
    "    covid_tweets = pd.read_csv(ROOT_DATA+'uhc_data.csv')\n",
    "except:\n",
    "    #call main method passing keywords and file path\n",
    "    write_tweets(uhc_keywords,  uhc_tweets)\n",
    "    covid_tweets = pd.read_csv(ROOT_DATA+'uhc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fix retweet IDs for media\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd63f3e4bee4d2192ba397e89c4f501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Remove duplicate records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ab540a41724ea3b84375913bc2cfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4707.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_tweets.retweeted = covid_tweets.retweeted.map(lambda x: '{:.0f}'.format(x))\n",
    "\n",
    "#Convert date to Y-M-d format\n",
    "covid_tweets.timestamp = covid_tweets.timestamp.apply(lambda x:time.strftime('%Y-%m-%d %H:%M:%S', \n",
    "                                                     time.strptime(x,'%a %b %d %H:%M:%S +0000 %Y')))\n",
    "\n",
    "fix_retweet_handle = covid_tweets[~covid_tweets.orig_tweet.isnull() &\n",
    "                                  covid_tweets.orig_tweet.str.startswith('RT @')]\n",
    "\n",
    "def add_retweet_id(status_id):\n",
    "    row = covid_tweets[covid_tweets.id==status_id]\n",
    "    orig_tweet = row.orig_tweet.unique().tolist()\n",
    "    if len(orig_tweet) > 1:\n",
    "        orig_tweet = [tweet for tweet in orig_tweet if pd.notnull(tweet)][0]\n",
    "    else:\n",
    "        orig_tweet = orig_tweet[0]\n",
    "    if ~pd.isna(orig_tweet):\n",
    "                \n",
    "        tweet_handle = re.findall('\\@(\\w+)',orig_tweet)[0]\n",
    "        \n",
    "        tweet = orig_tweet.replace('RT @'+tweet_handle+\": \",'')[0:30]\n",
    "        retweet_id = covid_tweets.id[(covid_tweets.tweeter_handle==tweet_handle)\n",
    "                    & covid_tweets.orig_tweet.str.contains(tweet)].unique().tolist()        \n",
    "        covid_tweets.loc[covid_tweets.id == status_id,['retweeted']]=\\\n",
    "        str(retweet_id[0]) if len(retweet_id) > 0 else np.nan    \n",
    "    return True\n",
    "\n",
    "print(\"\\nFix retweet IDs for media\")\n",
    "done = fix_retweet_handle.id.progress_apply(add_retweet_id)\n",
    "\n",
    "covid_tweets.hashtags[~covid_tweets.orig_tweet.isnull() \n",
    "                        & covid_tweets.orig_tweet.str.startswith('RT @')]= np.nan\n",
    "covid_tweets.mentions[~covid_tweets.orig_tweet.isnull() \n",
    "                        & covid_tweets.orig_tweet.str.startswith('RT @')]= np.nan\n",
    "covid_tweets.likes[~covid_tweets.orig_tweet.isnull() \n",
    "                        & covid_tweets.orig_tweet.str.startswith('RT @')]= 0\n",
    "covid_tweets.retweets[~covid_tweets.orig_tweet.isnull() \n",
    "                        & covid_tweets.orig_tweet.str.startswith('RT @')]= 0\n",
    "covid_tweets.orig_tweet[~covid_tweets.orig_tweet.isnull() \n",
    "                        & covid_tweets.orig_tweet.str.startswith('RT @')]= np.nan\n",
    "\n",
    "repeated_tweets = covid_tweets.groupby('id').tweeter_handle.count()\n",
    "repeated_tweets = repeated_tweets.to_frame()\n",
    "repeated_tweets.reset_index(inplace=True)\n",
    "repeated_tweets = repeated_tweets[repeated_tweets.tweeter_handle > 1]\n",
    "repeated_tweets.rename(columns={'tweeter_handle':'counts'},inplace=True)\n",
    "repeated_tweets.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "ok_tweets  = covid_tweets[~covid_tweets.id.isin(repeated_tweets.id)]\n",
    "tweets_to_clean  = covid_tweets[covid_tweets.id.isin(repeated_tweets.id)]\n",
    "tweets_to_clean = tweets_to_clean.sort_values(['id','tweeter_handle','timestamp'],ascending=False)\n",
    "ok_tweets = ok_tweets.sort_values(['id','tweeter_handle','timestamp'],ascending=False)\n",
    "\n",
    "\n",
    "def clean_tweet(tweet_id):    \n",
    "    \n",
    "    tweet = tweets_to_clean[tweets_to_clean.id==tweet_id]    \n",
    "    likes = tweet.likes.max()\n",
    "    retweets = tweet.retweets.max()\n",
    "    retweeted = tweet.retweeted.unique().tolist()\n",
    "    retweeted = [twerp for twerp in retweeted if twerp != 'nan']\n",
    "    retweeted = np.nan if len(retweeted)==0 else retweeted[0]\n",
    "    reply = np.nan if len(tweet.reply.unique().tolist())==0 else tweet.reply.unique().tolist()[0]\n",
    "    \n",
    "    this_tweet=pd.DataFrame({\n",
    "            'id':tweet_id\n",
    "            ,'tweeter_handle':tweet['tweeter_handle'].unique().tolist()[0]\n",
    "            ,'init_reach':tweet['init_reach'].unique().tolist()[0]\n",
    "            ,'timestamp':tweet['timestamp'].unique().tolist()[0]\n",
    "            ,'orig_tweet':tweet['orig_tweet'].unique().tolist()[0]                               \n",
    "            ,'likes':likes\n",
    "            ,'retweets':retweets \n",
    "            ,'hashtags':tweet['hashtags'].unique().tolist()[0]\n",
    "            ,'mentions':tweet['mentions'].unique().tolist()[0]\n",
    "            ,'location':tweet['location'].unique().tolist()[0]\n",
    "            ,'retweeted':retweeted\n",
    "            ,'reply':reply\n",
    "            \n",
    "    },index=[0])\n",
    "    return this_tweet\n",
    "\n",
    "print(\"\\nRemove duplicate records\")\n",
    "cleaned_tweets = repeated_tweets.id.progress_apply(clean_tweet)\n",
    "cleaned_tweets = pd.concat(cleaned_tweets.tolist(),axis=0)\n",
    "\n",
    "last_weeks_covid_tweets = pd.concat([ok_tweets,cleaned_tweets],axis=0)\n",
    "last_weeks_covid_tweets = last_weeks_covid_tweets.sort_values(['id','tweeter_handle','timestamp'],\n",
    "                                                              ascending=False)\n",
    "last_weeks_covid_tweets.reset_index(inplace=True,drop=True)\n",
    "last_weeks_covid_tweets.retweeted[last_weeks_covid_tweets.retweeted=='nan']=np.nan\n",
    "\n",
    "tweets = last_weeks_covid_tweets.id.unique().tolist()\n",
    "retweeted_tweets = last_weeks_covid_tweets.retweeted.unique().tolist()\n",
    "retweeted_tweets = [int(tweet) for tweet in retweeted_tweets if not pd.isnull(tweet)]\n",
    "unmatched_retweets = [str(tweet) for tweet in retweeted_tweets if tweet not in tweets]\n",
    "last_weeks_covid_tweets.retweeted[last_weeks_covid_tweets.retweeted.isin(unmatched_retweets) & \n",
    "                       ~pd.isnull(last_weeks_covid_tweets.orig_tweet)] = np.nan\n",
    "last_weeks_covid_tweets = last_weeks_covid_tweets[~last_weeks_covid_tweets.retweeted.isin(unmatched_retweets)]\n",
    "last_weeks_covid_tweets.to_csv(ROOT_DATA+\"covid_nlp_data.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
